file,method_name,new_method_name,start_line,end_line,original_code,code,var
./sklearn/model_selection/_search.py,fit,execute_cross_validation_fit,579,780,"def fit(self, X, y=None, groups=None, **fit_params):
    """"""Run fit with all sets of parameters.

        Parameters
        ----------

        X : array-like, shape = [n_samples, n_features]
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.

        y : array-like, shape = [n_samples] or [n_samples, n_output], optional
            Target relative to X for classification or regression;
            None for unsupervised learning.

        groups : array-like, with shape (n_samples,), optional
            Group labels for the samples used while splitting the dataset into
            train/test set.

        **fit_params : dict of string -> object
            Parameters passed to the ``fit`` method of the estimator
        """"""
    if self.fit_params is not None:
        warnings.warn('""fit_params"" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the ""fit"" method instead.', DeprecationWarning)
        if fit_params:
            warnings.warn('Ignoring fit_params passed as a constructor argument in favor of keyword arguments to the ""fit"" method.', RuntimeWarning)
        else:
            fit_params = self.fit_params
    estimator = self.estimator
    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
    scorers, self.multimetric_ = _check_multimetric_scoring(self.estimator, scoring=self.scoring)
    if self.multimetric_:
        if self.refit is not False and (not isinstance(self.refit, six.string_types) or self.refit not in scorers):
            raise ValueError('For multi-metric scoring, the parameter refit must be set to a scorer key to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not needed, refit should be set to False explicitly. %r was passed.' % self.refit)
        else:
            refit_metric = self.refit
    else:
        refit_metric = 'score'
    X, y, groups = indexable(X, y, groups)
    n_splits = cv.get_n_splits(X, y, groups)
    candidate_params = list(self._get_param_iterator())
    n_candidates = len(candidate_params)
    if self.verbose > 0:
        print('Fitting {0} folds for each of {1} candidates, totalling {2} fits'.format(n_splits, n_candidates, n_candidates * n_splits))
    base_estimator = clone(self.estimator)
    pre_dispatch = self.pre_dispatch
    out = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, pre_dispatch=pre_dispatch)((delayed(_fit_and_score)(clone(base_estimator), X, y, scorers, train, test, self.verbose, parameters, fit_params=fit_params, return_train_score=self.return_train_score, return_n_test_samples=True, return_times=True, return_parameters=False, error_score=self.error_score) for parameters, (train, test) in product(candidate_params, cv.split(X, y, groups))))
    if self.return_train_score:
        train_score_dicts, test_score_dicts, test_sample_counts, fit_time, score_time = zip(*out)
    else:
        test_score_dicts, test_sample_counts, fit_time, score_time = zip(*out)
    test_scores = _aggregate_score_dicts(test_score_dicts)
    if self.return_train_score:
        train_scores = _aggregate_score_dicts(train_score_dicts)
    results = DeprecationDict() if self.return_train_score == 'warn' else {}

    def _store(key_name, array, weights=None, splits=False, rank=False):
        """"""A small helper to store the scores/times to the cv_results_""""""
        array = np.array(array, dtype=np.float64).reshape(n_candidates, n_splits)
        if splits:
            for split_i in range(n_splits):
                results['split%d_%s' % (split_i, key_name)] = array[:, split_i]
        array_means = np.average(array, axis=1, weights=weights)
        results['mean_%s' % key_name] = array_means
        array_stds = np.sqrt(np.average((array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights))
        results['std_%s' % key_name] = array_stds
        if rank:
            results['rank_%s' % key_name] = np.asarray(rankdata(-array_means, method='min'), dtype=np.int32)
    _store('fit_time', fit_time)
    _store('score_time', score_time)
    param_results = defaultdict(partial(MaskedArray, np.empty(n_candidates), mask=True, dtype=object))
    for cand_i, params in enumerate(candidate_params):
        for name, value in params.items():
            param_results['param_%s' % name][cand_i] = value
    results.update(param_results)
    results['params'] = candidate_params
    test_sample_counts = np.array(test_sample_counts[:n_splits], dtype=np.int)
    iid = self.iid
    if self.iid == 'warn':
        if len(np.unique(test_sample_counts)) > 1:
            warnings.warn('The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.', DeprecationWarning)
        iid = True
    for scorer_name in scorers.keys():
        _store('test_%s' % scorer_name, test_scores[scorer_name], splits=True, rank=True, weights=test_sample_counts if iid else None)
        if self.return_train_score:
            prev_keys = set(results.keys())
            _store('train_%s' % scorer_name, train_scores[scorer_name], splits=True)
            if self.return_train_score == 'warn':
                for key in set(results.keys()) - prev_keys:
                    message = 'You are accessing a training score ({!r}), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True'.format(key)
                    results.add_warning(key, message, FutureWarning)
    if self.refit or not self.multimetric_:
        self.best_index_ = results['rank_test_%s' % refit_metric].argmin()
        self.best_params_ = candidate_params[self.best_index_]
        self.best_score_ = results['mean_test_%s' % refit_metric][self.best_index_]
    if self.refit:
        self.best_estimator_ = clone(base_estimator).set_params(**self.best_params_)
        if y is not None:
            self.best_estimator_.fit(X, y, **fit_params)
        else:
            self.best_estimator_.fit(X, **fit_params)
    self.scorer_ = scorers if self.multimetric_ else scorers['score']
    self.cv_results_ = results
    self.n_splits_ = n_splits
    return self","def execute_cross_validation_fit(self, X, y=None, groups=None, **fit_params):
    """"""Run fit with all sets of parameters.

        Parameters
        ----------

        X : array-like, shape = [n_samples, n_features]
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.

        y : array-like, shape = [n_samples] or [n_samples, n_output], optional
            Target relative to X for classification or regression;
            None for unsupervised learning.

        groups : array-like, with shape (n_samples,), optional
            Group labels for the samples used while splitting the dataset into
            train/test set.

        **fit_params : dict of string -> object
            Parameters passed to the ``fit`` method of the estimator
        """"""
    if self.fit_params is not None:
        warnings.warn('""fit_params"" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the ""fit"" method instead.', DeprecationWarning)
        if fit_params:
            warnings.warn('Ignoring fit_params passed as a constructor argument in favor of keyword arguments to the ""fit"" method.', RuntimeWarning)
        else:
            fit_params = self.fit_params
    estimator = self.estimator
    cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
    scorers, self.multimetric_ = _check_multimetric_scoring(self.estimator, scoring=self.scoring)
    if self.multimetric_:
        if self.refit is not False and (not isinstance(self.refit, six.string_types) or self.refit not in scorers):
            raise ValueError('For multi-metric scoring, the parameter refit must be set to a scorer key to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric. If this is not needed, refit should be set to False explicitly. %r was passed.' % self.refit)
        else:
            refit_metric = self.refit
    else:
        refit_metric = 'score'
    X, y, groups = indexable(X, y, groups)
    n_splits = cv.get_n_splits(X, y, groups)
    candidate_params = list(self._get_param_iterator())
    n_candidates = len(candidate_params)
    if self.verbose > 0:
        print('Fitting {0} folds for each of {1} candidates, totalling {2} fits'.format(n_splits, n_candidates, n_candidates * n_splits))
    base_estimator = clone(self.estimator)
    pre_dispatch = self.pre_dispatch
    out = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, pre_dispatch=pre_dispatch)((delayed(_fit_and_score)(clone(base_estimator), X, y, scorers, train, test, self.verbose, parameters, fit_params=fit_params, return_train_score=self.return_train_score, return_n_test_samples=True, return_times=True, return_parameters=False, error_score=self.error_score) for parameters, (train, test) in product(candidate_params, cv.split(X, y, groups))))
    if self.return_train_score:
        train_score_dicts, test_score_dicts, test_sample_counts, fit_time, score_time = zip(*out)
    else:
        test_score_dicts, test_sample_counts, fit_time, score_time = zip(*out)
    test_scores = _aggregate_score_dicts(test_score_dicts)
    if self.return_train_score:
        train_scores = _aggregate_score_dicts(train_score_dicts)
    results = DeprecationDict() if self.return_train_score == 'warn' else {}

    def _store(key_name, array, weights=None, splits=False, rank=False):
        """"""A small helper to store the scores/times to the cv_results_""""""
        array = np.array(array, dtype=np.float64).reshape(n_candidates, n_splits)
        if splits:
            for split_i in range(n_splits):
                results['split%d_%s' % (split_i, key_name)] = array[:, split_i]
        array_means = np.average(array, axis=1, weights=weights)
        results['mean_%s' % key_name] = array_means
        array_stds = np.sqrt(np.average((array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights))
        results['std_%s' % key_name] = array_stds
        if rank:
            results['rank_%s' % key_name] = np.asarray(rankdata(-array_means, method='min'), dtype=np.int32)
    _store('fit_time', fit_time)
    _store('score_time', score_time)
    param_results = defaultdict(partial(MaskedArray, np.empty(n_candidates), mask=True, dtype=object))
    for cand_i, params in enumerate(candidate_params):
        for name, value in params.items():
            param_results['param_%s' % name][cand_i] = value
    results.update(param_results)
    results['params'] = candidate_params
    test_sample_counts = np.array(test_sample_counts[:n_splits], dtype=np.int)
    iid = self.iid
    if self.iid == 'warn':
        if len(np.unique(test_sample_counts)) > 1:
            warnings.warn('The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.', DeprecationWarning)
        iid = True
    for scorer_name in scorers.keys():
        _store('test_%s' % scorer_name, test_scores[scorer_name], splits=True, rank=True, weights=test_sample_counts if iid else None)
        if self.return_train_score:
            prev_keys = set(results.keys())
            _store('train_%s' % scorer_name, train_scores[scorer_name], splits=True)
            if self.return_train_score == 'warn':
                for key in set(results.keys()) - prev_keys:
                    message = 'You are accessing a training score ({!r}), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True'.format(key)
                    results.add_warning(key, message, FutureWarning)
    if self.refit or not self.multimetric_:
        self.best_index_ = results['rank_test_%s' % refit_metric].argmin()
        self.best_params_ = candidate_params[self.best_index_]
        self.best_score_ = results['mean_test_%s' % refit_metric][self.best_index_]
    if self.refit:
        self.best_estimator_ = clone(base_estimator).set_params(**self.best_params_)
        if y is not None:
            self.best_estimator_.fit(X, y, **fit_params)
        else:
            self.best_estimator_.fit(X, **fit_params)
    self.scorer_ = scorers if self.multimetric_ else scorers['score']
    self.cv_results_ = results
    self.n_splits_ = n_splits
    return self","[{""var"": ""out"", ""rename"": ""fit_score_results""}, {""var"": ""n_candidates"", ""rename"": ""total_candidate_params""}, {""var"": ""params"", ""rename"": ""hyperparameter_settings""}, {""var"": ""key"", ""rename"": ""scorer_key_name""}, {""var"": ""message"", ""rename"": ""deprecation_warning_message""}, {""var"": ""split_i"", ""rename"": ""evaluation_split_index""}, {""var"": ""base_estimator"", ""rename"": ""cloned_model_estimator""}, {""var"": ""score_time"", ""rename"": ""scoring_duration""}, {""var"": ""estimator"", ""rename"": ""model_estimator""}, {""var"": ""name"", ""rename"": ""training_process_manager""}, {""var"": ""candidate_params"", ""rename"": ""parameter_candidates""}, {""var"": ""scorers"", ""rename"": ""evaluation_metrics""}, {""var"": ""train"", ""rename"": ""train_indices""}, {""var"": ""param_results"", ""rename"": ""candidate_param_values""}, {""var"": ""array_stds"", ""rename"": ""standard_deviations_array""}, {""var"": ""cand_i"", ""rename"": ""candidate_index""}, {""var"": ""train_score_dicts"", ""rename"": ""training_score_details""}, {""var"": ""cv"", ""rename"": ""cross_validator""}, {""var"": ""iid"", ""rename"": ""independent_and_identically_distributed""}, {""var"": ""test_score_dicts"", ""rename"": ""evaluation_score_details""}, {""var"": ""n_splits"", ""rename"": ""num_cross_validation_folds""}, {""var"": ""train_scores"", ""rename"": ""training_evaluation_scores""}, {""var"": ""fit_time"", ""rename"": ""fitting_duration""}, {""var"": ""value"", ""rename"": ""hyperparameter_value""}, {""var"": ""test_sample_counts"", ""rename"": ""test_sample_sizes""}, {""var"": ""results"", ""rename"": ""cross_validation_results""}, {""var"": ""test"", ""rename"": ""test_indices""}, {""var"": ""refit_metric"", ""rename"": ""refit_scorer_key""}, {""var"": ""parameters"", ""rename"": ""hyperparameter_combinations""}, {""var"": ""array_means"", ""rename"": ""average_scores_array""}, {""var"": ""scorer_name"", ""rename"": ""evaluation_metric_name""}, {""var"": ""test_scores"", ""rename"": ""cross_validation_test_scores""}, {""var"": ""prev_keys"", ""rename"": ""previous_result_keys""}, {""var"": ""pre_dispatch"", ""rename"": ""precomputed_dispatch""}]"
