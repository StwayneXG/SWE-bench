file,method_name,new_method_name,start_line,end_line,original_code,code,var
./sklearn/feature_selection/_sequential.py,fit,select_features_based_on_criteria,189,292,"def fit(self, X, y=None):
    """"""Learn the features to select from X.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training vectors, where `n_samples` is the number of samples and
            `n_features` is the number of predictors.

        y : array-like of shape (n_samples,), default=None
            Target values. This parameter may be ignored for
            unsupervised learning.

        Returns
        -------
        self : object
            Returns the instance itself.
        """"""
    self._validate_params()
    if self.n_features_to_select in ('warn', None):
        warnings.warn(""Leaving `n_features_to_select` to None is deprecated in 1.0 and will become 'auto' in 1.3. To keep the same behaviour as with None (i.e. select half of the features) and avoid this warning, you should manually set `n_features_to_select='auto'` and set tol=None when creating an instance."", FutureWarning)
    tags = self._get_tags()
    X = self._validate_data(X, accept_sparse='csc', ensure_min_features=2, force_all_finite=not tags.get('allow_nan', True))
    n_features = X.shape[1]
    error_msg = f""n_features_to_select must be either 'auto', 'warn', None, an integer in [1, n_features - 1] representing the absolute number of features, or a float in (0, 1] representing a percentage of features to select. Got {self.n_features_to_select}""
    if self.n_features_to_select in ('warn', None):
        if self.tol is not None:
            raise ValueError(""tol is only enabled if `n_features_to_select='auto'`"")
        self.n_features_to_select_ = n_features // 2
    elif self.n_features_to_select == 'auto':
        if self.tol is not None:
            self.n_features_to_select_ = n_features - 1
        else:
            self.n_features_to_select_ = n_features // 2
    elif isinstance(self.n_features_to_select, Integral):
        if not 0 < self.n_features_to_select < n_features:
            raise ValueError(error_msg)
        self.n_features_to_select_ = self.n_features_to_select
    elif isinstance(self.n_features_to_select, Real):
        self.n_features_to_select_ = int(n_features * self.n_features_to_select)
    if self.tol is not None and self.tol < 0 and (self.direction == 'forward'):
        raise ValueError('tol must be positive when doing forward selection')
    cloned_estimator = clone(self.estimator)
    current_mask = np.zeros(shape=n_features, dtype=bool)
    n_iterations = self.n_features_to_select_ if self.n_features_to_select == 'auto' or self.direction == 'forward' else n_features - self.n_features_to_select_
    old_score = -np.inf
    is_auto_select = self.tol is not None and self.n_features_to_select == 'auto'
    for _ in range(n_iterations):
        new_feature_idx, new_score = self._get_best_new_feature_score(cloned_estimator, X, y, current_mask)
        if is_auto_select and new_score - old_score < self.tol:
            break
        old_score = new_score
        current_mask[new_feature_idx] = True
    if self.direction == 'backward':
        current_mask = ~current_mask
    self.support_ = current_mask
    self.n_features_to_select_ = self.support_.sum()
    return self","def select_features_based_on_criteria(self, X, y=None):
    """"""Learn the features to select from X.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training vectors, where `n_samples` is the number of samples and
            `n_features` is the number of predictors.

        y : array-like of shape (n_samples,), default=None
            Target values. This parameter may be ignored for
            unsupervised learning.

        Returns
        -------
        self : object
            Returns the instance itself.
        """"""
    self._validate_params()
    if self.n_features_to_select in ('warn', None):
        warnings.warn(""Leaving `n_features_to_select` to None is deprecated in 1.0 and will become 'auto' in 1.3. To keep the same behaviour as with None (i.e. select half of the features) and avoid this warning, you should manually set `n_features_to_select='auto'` and set tol=None when creating an instance."", FutureWarning)
    tags = self._get_tags()
    X = self._validate_data(X, accept_sparse='csc', ensure_min_features=2, force_all_finite=not tags.get('allow_nan', True))
    n_features = X.shape[1]
    error_msg = f""n_features_to_select must be either 'auto', 'warn', None, an integer in [1, n_features - 1] representing the absolute number of features, or a float in (0, 1] representing a percentage of features to select. Got {self.n_features_to_select}""
    if self.n_features_to_select in ('warn', None):
        if self.tol is not None:
            raise ValueError(""tol is only enabled if `n_features_to_select='auto'`"")
        self.n_features_to_select_ = n_features // 2
    elif self.n_features_to_select == 'auto':
        if self.tol is not None:
            self.n_features_to_select_ = n_features - 1
        else:
            self.n_features_to_select_ = n_features // 2
    elif isinstance(self.n_features_to_select, Integral):
        if not 0 < self.n_features_to_select < n_features:
            raise ValueError(error_msg)
        self.n_features_to_select_ = self.n_features_to_select
    elif isinstance(self.n_features_to_select, Real):
        self.n_features_to_select_ = int(n_features * self.n_features_to_select)
    if self.tol is not None and self.tol < 0 and (self.direction == 'forward'):
        raise ValueError('tol must be positive when doing forward selection')
    cloned_estimator = clone(self.estimator)
    current_mask = np.zeros(shape=n_features, dtype=bool)
    n_iterations = self.n_features_to_select_ if self.n_features_to_select == 'auto' or self.direction == 'forward' else n_features - self.n_features_to_select_
    old_score = -np.inf
    is_auto_select = self.tol is not None and self.n_features_to_select == 'auto'
    for _ in range(n_iterations):
        new_feature_idx, new_score = self._get_best_new_feature_score(cloned_estimator, X, y, current_mask)
        if is_auto_select and new_score - old_score < self.tol:
            break
        old_score = new_score
        current_mask[new_feature_idx] = True
    if self.direction == 'backward':
        current_mask = ~current_mask
    self.support_ = current_mask
    self.n_features_to_select_ = self.support_.sum()
    return self","[{""var"": ""n_iterations"", ""rename"": ""num_selection_iterations""}, {""var"": ""_"", ""rename"": ""selection_iteration""}, {""var"": ""new_feature_idx"", ""rename"": ""selected_feature_index""}, {""var"": ""new_score"", ""rename"": ""current_iteration_score""}, {""var"": ""old_score"", ""rename"": ""previous_iteration_score""}, {""var"": ""n_features"", ""rename"": ""num_predictor_features""}, {""var"": ""current_mask"", ""rename"": ""feature_selection_mask""}, {""var"": ""tags"", ""rename"": ""feature_selection_tags""}, {""var"": ""cloned_estimator"", ""rename"": ""replicated_model""}, {""var"": ""is_auto_select"", ""rename"": ""is_auto_selection_enabled""}, {""var"": ""error_msg"", ""rename"": ""n_features_error_message""}]"
./sklearn/feature_selection/_sequential.py,_get_best_new_feature_score,evaluate_and_select_optimal_feature,294,317,"def _get_best_new_feature_score(self, estimator, X, y, current_mask):
    candidate_feature_indices = np.flatnonzero(~current_mask)
    scores = {}
    for feature_idx in candidate_feature_indices:
        candidate_mask = current_mask.copy()
        candidate_mask[feature_idx] = True
        if self.direction == 'backward':
            candidate_mask = ~candidate_mask
        X_new = X[:, candidate_mask]
        scores[feature_idx] = cross_val_score(estimator, X_new, y, cv=self.cv, scoring=self.scoring, n_jobs=self.n_jobs).mean()
    new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])
    return (new_feature_idx, scores[new_feature_idx])","def evaluate_and_select_optimal_feature(self, estimator, X, y, current_mask):
    candidate_feature_indices = np.flatnonzero(~current_mask)
    scores = {}
    for feature_idx in candidate_feature_indices:
        candidate_mask = current_mask.copy()
        candidate_mask[feature_idx] = True
        if self.direction == 'backward':
            candidate_mask = ~candidate_mask
        X_new = X[:, candidate_mask]
        scores[feature_idx] = cross_val_score(estimator, X_new, y, cv=self.cv, scoring=self.scoring, n_jobs=self.n_jobs).mean()
    new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])
    return (new_feature_idx, scores[new_feature_idx])","[{""var"": ""feature_idx"", ""rename"": ""candidate_feature_index""}, {""var"": ""new_feature_idx"", ""rename"": ""optimal_candidate_feature_index""}, {""var"": ""X_new"", ""rename"": ""feature_subset_matrix""}, {""var"": ""candidate_feature_indices"", ""rename"": ""potential_feature_indices_for_selection""}, {""var"": ""candidate_mask"", ""rename"": ""updated_feature_selection_mask""}, {""var"": ""scores"", ""rename"": ""feature_performance_scores""}]"
