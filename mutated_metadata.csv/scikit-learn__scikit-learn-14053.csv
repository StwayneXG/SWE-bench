file,method_name,new_method_name,start_line,end_line,original_code,code,var
./sklearn/tree/export.py,export_text,generate_decision_tree_report,806,957,"def export_text(decision_tree, feature_names=None, max_depth=10, spacing=3, decimals=2, show_weights=False):
    """"""Build a text report showing the rules of a decision tree.

    Note that backwards compatibility may not be supported.

    Parameters
    ----------
    decision_tree : object
        The decision tree estimator to be exported.
        It can be an instance of
        DecisionTreeClassifier or DecisionTreeRegressor.

    feature_names : list, optional (default=None)
        A list of length n_features containing the feature names.
        If None generic names will be used (""feature_0"", ""feature_1"", ...).

    max_depth : int, optional (default=10)
        Only the first max_depth levels of the tree are exported.
        Truncated branches will be marked with ""..."".

    spacing : int, optional (default=3)
        Number of spaces between edges. The higher it is, the wider the result.

    decimals : int, optional (default=2)
        Number of decimal digits to display.

    show_weights : bool, optional (default=False)
        If true the classification weights will be exported on each leaf.
        The classification weights are the number of samples each class.

    Returns
    -------
    report : string
        Text summary of all the rules in the decision tree.

    Examples
    --------

    >>> from sklearn.datasets import load_iris
    >>> from sklearn.tree import DecisionTreeClassifier
    >>> from sklearn.tree.export import export_text
    >>> iris = load_iris()
    >>> X = iris['data']
    >>> y = iris['target']
    >>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)
    >>> decision_tree = decision_tree.fit(X, y)
    >>> r = export_text(decision_tree, feature_names=iris['feature_names'])
    >>> print(r)
    |--- petal width (cm) <= 0.80
    |   |--- class: 0
    |--- petal width (cm) >  0.80
    |   |--- petal width (cm) <= 1.75
    |   |   |--- class: 1
    |   |--- petal width (cm) >  1.75
    |   |   |--- class: 2
    """"""
    check_is_fitted(decision_tree, 'tree_')
    tree_ = decision_tree.tree_
    class_names = decision_tree.classes_
    right_child_fmt = '{} {} <= {}\n'
    left_child_fmt = '{} {} >  {}\n'
    truncation_fmt = '{} {}\n'
    if max_depth < 0:
        raise ValueError('max_depth bust be >= 0, given %d' % max_depth)
    if feature_names is not None and len(feature_names) != tree_.n_features:
        raise ValueError('feature_names must contain %d elements, got %d' % (tree_.n_features, len(feature_names)))
    if spacing <= 0:
        raise ValueError('spacing must be > 0, given %d' % spacing)
    if decimals < 0:
        raise ValueError('decimals must be >= 0, given %d' % decimals)
    if isinstance(decision_tree, DecisionTreeClassifier):
        value_fmt = '{}{} weights: {}\n'
        if not show_weights:
            value_fmt = '{}{}{}\n'
    else:
        value_fmt = '{}{} value: {}\n'
    if feature_names:
        feature_names_ = [feature_names[i] for i in tree_.feature]
    else:
        feature_names_ = ['feature_{}'.format(i) for i in tree_.feature]
    export_text.report = ''

    def _add_leaf(value, class_name, indent):
        val = ''
        is_classification = isinstance(decision_tree, DecisionTreeClassifier)
        if show_weights or not is_classification:
            val = ['{1:.{0}f}, '.format(decimals, v) for v in value]
            val = '[' + ''.join(val)[:-2] + ']'
        if is_classification:
            val += ' class: ' + str(class_name)
        export_text.report += value_fmt.format(indent, '', val)

    def print_tree_recurse(node, depth):
        indent = ('|' + ' ' * spacing) * depth
        indent = indent[:-spacing] + '-' * spacing
        value = None
        if tree_.n_outputs == 1:
            value = tree_.value[node][0]
        else:
            value = tree_.value[node].T[0]
        class_name = np.argmax(value)
        if tree_.n_classes[0] != 1 and tree_.n_outputs == 1:
            class_name = class_names[class_name]
        if depth <= max_depth + 1:
            info_fmt = ''
            info_fmt_left = info_fmt
            info_fmt_right = info_fmt
            if tree_.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_names_[node]
                threshold = tree_.threshold[node]
                threshold = '{1:.{0}f}'.format(decimals, threshold)
                export_text.report += right_child_fmt.format(indent, name, threshold)
                export_text.report += info_fmt_left
                print_tree_recurse(tree_.children_left[node], depth + 1)
                export_text.report += left_child_fmt.format(indent, name, threshold)
                export_text.report += info_fmt_right
                print_tree_recurse(tree_.children_right[node], depth + 1)
            else:
                _add_leaf(value, class_name, indent)
        else:
            subtree_depth = _compute_depth(tree_, node)
            if subtree_depth == 1:
                _add_leaf(value, class_name, indent)
            else:
                trunc_report = 'truncated branch of depth %d' % subtree_depth
                export_text.report += truncation_fmt.format(indent, trunc_report)
    print_tree_recurse(0, 1)
    return export_text.report","def generate_decision_tree_report(decision_tree, feature_names=None, max_depth=10, spacing=3, decimals=2, show_weights=False):
    """"""Build a text report showing the rules of a decision tree.

    Note that backwards compatibility may not be supported.

    Parameters
    ----------
    decision_tree : object
        The decision tree estimator to be exported.
        It can be an instance of
        DecisionTreeClassifier or DecisionTreeRegressor.

    feature_names : list, optional (default=None)
        A list of length n_features containing the feature names.
        If None generic names will be used (""feature_0"", ""feature_1"", ...).

    max_depth : int, optional (default=10)
        Only the first max_depth levels of the tree are exported.
        Truncated branches will be marked with ""..."".

    spacing : int, optional (default=3)
        Number of spaces between edges. The higher it is, the wider the result.

    decimals : int, optional (default=2)
        Number of decimal digits to display.

    show_weights : bool, optional (default=False)
        If true the classification weights will be exported on each leaf.
        The classification weights are the number of samples each class.

    Returns
    -------
    report : string
        Text summary of all the rules in the decision tree.

    Examples
    --------

    >>> from sklearn.datasets import load_iris
    >>> from sklearn.tree import DecisionTreeClassifier
    >>> from sklearn.tree.export import export_text
    >>> iris = load_iris()
    >>> X = iris['data']
    >>> y = iris['target']
    >>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)
    >>> decision_tree = decision_tree.fit(X, y)
    >>> r = export_text(decision_tree, feature_names=iris['feature_names'])
    >>> print(r)
    |--- petal width (cm) <= 0.80
    |   |--- class: 0
    |--- petal width (cm) >  0.80
    |   |--- petal width (cm) <= 1.75
    |   |   |--- class: 1
    |   |--- petal width (cm) >  1.75
    |   |   |--- class: 2
    """"""
    check_is_fitted(decision_tree, 'tree_')
    tree_ = decision_tree.tree_
    class_names = decision_tree.classes_
    right_child_fmt = '{} {} <= {}\n'
    left_child_fmt = '{} {} >  {}\n'
    truncation_fmt = '{} {}\n'
    if max_depth < 0:
        raise ValueError('max_depth bust be >= 0, given %d' % max_depth)
    if feature_names is not None and len(feature_names) != tree_.n_features:
        raise ValueError('feature_names must contain %d elements, got %d' % (tree_.n_features, len(feature_names)))
    if spacing <= 0:
        raise ValueError('spacing must be > 0, given %d' % spacing)
    if decimals < 0:
        raise ValueError('decimals must be >= 0, given %d' % decimals)
    if isinstance(decision_tree, DecisionTreeClassifier):
        value_fmt = '{}{} weights: {}\n'
        if not show_weights:
            value_fmt = '{}{}{}\n'
    else:
        value_fmt = '{}{} value: {}\n'
    if feature_names:
        feature_names_ = [feature_names[i] for i in tree_.feature]
    else:
        feature_names_ = ['feature_{}'.format(i) for i in tree_.feature]
    export_text.report = ''

    def _add_leaf(value, class_name, indent):
        val = ''
        is_classification = isinstance(decision_tree, DecisionTreeClassifier)
        if show_weights or not is_classification:
            val = ['{1:.{0}f}, '.format(decimals, v) for v in value]
            val = '[' + ''.join(val)[:-2] + ']'
        if is_classification:
            val += ' class: ' + str(class_name)
        export_text.report += value_fmt.format(indent, '', val)

    def print_tree_recurse(node, depth):
        indent = ('|' + ' ' * spacing) * depth
        indent = indent[:-spacing] + '-' * spacing
        value = None
        if tree_.n_outputs == 1:
            value = tree_.value[node][0]
        else:
            value = tree_.value[node].T[0]
        class_name = np.argmax(value)
        if tree_.n_classes[0] != 1 and tree_.n_outputs == 1:
            class_name = class_names[class_name]
        if depth <= max_depth + 1:
            info_fmt = ''
            info_fmt_left = info_fmt
            info_fmt_right = info_fmt
            if tree_.feature[node] != _tree.TREE_UNDEFINED:
                name = feature_names_[node]
                threshold = tree_.threshold[node]
                threshold = '{1:.{0}f}'.format(decimals, threshold)
                export_text.report += right_child_fmt.format(indent, name, threshold)
                export_text.report += info_fmt_left
                print_tree_recurse(tree_.children_left[node], depth + 1)
                export_text.report += left_child_fmt.format(indent, name, threshold)
                export_text.report += info_fmt_right
                print_tree_recurse(tree_.children_right[node], depth + 1)
            else:
                _add_leaf(value, class_name, indent)
        else:
            subtree_depth = _compute_depth(tree_, node)
            if subtree_depth == 1:
                _add_leaf(value, class_name, indent)
            else:
                trunc_report = 'truncated branch of depth %d' % subtree_depth
                export_text.report += truncation_fmt.format(indent, trunc_report)
    print_tree_recurse(0, 1)
    return export_text.report","[{""var"": ""val"", ""rename"": ""formatted_value""}, {""var"": ""is_classification"", ""rename"": ""is_decision_tree_classifier""}, {""var"": ""info_fmt_left"", ""rename"": ""indentation_info_left""}, {""var"": ""name"", ""rename"": ""feature_name_at_node""}, {""var"": ""threshold"", ""rename"": ""decision_threshold""}, {""var"": ""i"", ""rename"": ""feature_index""}, {""var"": ""trunc_report"", ""rename"": ""truncated_branch_description""}, {""var"": ""value_fmt"", ""rename"": ""node_value_format""}, {""var"": ""class_names"", ""rename"": ""target_class_labels""}, {""var"": ""feature_names_"", ""rename"": ""tree_feature_identifiers""}, {""var"": ""info_fmt"", ""rename"": ""info_message_format""}, {""var"": ""v"", ""rename"": ""node_value""}, {""var"": ""info_fmt_right"", ""rename"": ""right_subtree_info""}, {""var"": ""truncation_fmt"", ""rename"": ""truncation_placeholder""}, {""var"": ""left_child_fmt"", ""rename"": ""left_branch_format""}, {""var"": ""subtree_depth"", ""rename"": ""remaining_branch_depth""}, {""var"": ""tree_"", ""rename"": ""decision_tree_structure""}, {""var"": ""right_child_fmt"", ""rename"": ""right_branch_format""}]"
