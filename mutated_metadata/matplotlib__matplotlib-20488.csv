file,method_name,new_method_name,start_line,end_line,original_code,code,var
./lib/matplotlib/image.py,_make_image,normalize_rescale_and_transform_image,307,584,"def _make_image(self, A, in_bbox, out_bbox, clip_bbox, magnification=1.0, unsampled=False, round_to_pixel_border=True):
    """"""
        Normalize, rescale, and colormap the image *A* from the given *in_bbox*
        (in data space), to the given *out_bbox* (in pixel space) clipped to
        the given *clip_bbox* (also in pixel space), and magnified by the
        *magnification* factor.

        *A* may be a greyscale image (M, N) with a dtype of float32, float64,
        float128, uint16 or uint8, or an (M, N, 4) RGBA image with a dtype of
        float32, float64, float128, or uint8.

        If *unsampled* is True, the image will not be scaled, but an
        appropriate affine transformation will be returned instead.

        If *round_to_pixel_border* is True, the output image size will be
        rounded to the nearest pixel boundary.  This makes the images align
        correctly with the axes.  It should not be used if exact scaling is
        needed, such as for `FigureImage`.

        Returns
        -------
        image : (M, N, 4) uint8 array
            The RGBA image, resampled unless *unsampled* is True.
        x, y : float
            The upper left corner where the image should be drawn, in pixel
            space.
        trans : Affine2D
            The affine transformation from image to pixel space.
        """"""
    if A is None:
        raise RuntimeError('You must first set the image array or the image attribute')
    if A.size == 0:
        raise RuntimeError(""_make_image must get a non-empty image. Your Artist's draw method must filter before this method is called."")
    clipped_bbox = Bbox.intersection(out_bbox, clip_bbox)
    if clipped_bbox is None:
        return (None, 0, 0, None)
    out_width_base = clipped_bbox.width * magnification
    out_height_base = clipped_bbox.height * magnification
    if out_width_base == 0 or out_height_base == 0:
        return (None, 0, 0, None)
    if self.origin == 'upper':
        t0 = Affine2D().translate(0, -A.shape[0]).scale(1, -1)
    else:
        t0 = IdentityTransform()
    t0 += Affine2D().scale(in_bbox.width / A.shape[1], in_bbox.height / A.shape[0]).translate(in_bbox.x0, in_bbox.y0) + self.get_transform()
    t = t0 + Affine2D().translate(-clipped_bbox.x0, -clipped_bbox.y0).scale(magnification)
    if t.is_affine and round_to_pixel_border and (out_width_base % 1.0 != 0.0 or out_height_base % 1.0 != 0.0):
        out_width = math.ceil(out_width_base)
        out_height = math.ceil(out_height_base)
        extra_width = (out_width - out_width_base) / out_width_base
        extra_height = (out_height - out_height_base) / out_height_base
        t += Affine2D().scale(1.0 + extra_width, 1.0 + extra_height)
    else:
        out_width = int(out_width_base)
        out_height = int(out_height_base)
    out_shape = (out_height, out_width)
    if not unsampled:
        if not (A.ndim == 2 or (A.ndim == 3 and A.shape[-1] in (3, 4))):
            raise ValueError(f'Invalid shape {A.shape} for image data')
        if A.ndim == 2:
            inp_dtype = A.dtype
            a_min = A.min()
            a_max = A.max()
            if a_min is np.ma.masked:
                a_min, a_max = (np.int32(0), np.int32(1))
            if inp_dtype.kind == 'f':
                scaled_dtype = np.dtype(np.float64 if A.dtype.itemsize > 4 else np.float32)
                if scaled_dtype.itemsize < A.dtype.itemsize:
                    _api.warn_external(f'Casting input data from {A.dtype} to {scaled_dtype} for imshow')
            else:
                da = a_max.astype(np.float64) - a_min.astype(np.float64)
                scaled_dtype = np.float64 if da > 100000000.0 else np.float32
            A_scaled = np.array(A, dtype=scaled_dtype)
            self.norm.autoscale_None(A)
            dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)
            vmid = np.float64(self.norm.vmin) + dv / 2
            fact = 10000000.0 if scaled_dtype == np.float64 else 10000.0
            newmin = vmid - dv * fact
            if newmin < a_min:
                newmin = None
            else:
                a_min = np.float64(newmin)
            newmax = vmid + dv * fact
            if newmax > a_max:
                newmax = None
            else:
                a_max = np.float64(newmax)
            if newmax is not None or newmin is not None:
                np.clip(A_scaled, newmin, newmax, out=A_scaled)
            offset = 0.1
            frac = 0.8
            vmin, vmax = (self.norm.vmin, self.norm.vmax)
            if vmin is np.ma.masked:
                vmin, vmax = (a_min, a_max)
            vrange = np.array([vmin, vmax], dtype=scaled_dtype)
            A_scaled -= a_min
            vrange -= a_min
            a_min = a_min.astype(scaled_dtype).item()
            a_max = a_max.astype(scaled_dtype).item()
            if a_min != a_max:
                A_scaled /= (a_max - a_min) / frac
                vrange /= (a_max - a_min) / frac
            A_scaled += offset
            vrange += offset
            A_resampled = _resample(self, A_scaled, out_shape, t)
            del A_scaled
            A_resampled -= offset
            vrange -= offset
            if a_min != a_max:
                A_resampled *= (a_max - a_min) / frac
                vrange *= (a_max - a_min) / frac
            A_resampled += a_min
            vrange += a_min
            if isinstance(self.norm, mcolors.NoNorm):
                A_resampled = A_resampled.astype(A.dtype)
            mask = np.where(A.mask, np.float32(np.nan), np.float32(1)) if A.mask.shape == A.shape else np.ones_like(A, np.float32)
            out_alpha = _resample(self, mask, out_shape, t, resample=True)
            del mask
            out_mask = np.isnan(out_alpha)
            out_alpha[out_mask] = 1
            alpha = self.get_alpha()
            if alpha is not None and np.ndim(alpha) > 0:
                out_alpha *= _resample(self, alpha, out_shape, t, resample=True)
            resampled_masked = np.ma.masked_array(A_resampled, out_mask)
            s_vmin, s_vmax = vrange
            if isinstance(self.norm, mcolors.LogNorm):
                if s_vmin < 0:
                    s_vmin = max(s_vmin, np.finfo(scaled_dtype).eps)
            with cbook._setattr_cm(self.norm, vmin=s_vmin, vmax=s_vmax):
                output = self.norm(resampled_masked)
        else:
            if A.shape[2] == 3:
                A = _rgb_to_rgba(A)
            alpha = self._get_scalar_alpha()
            output_alpha = _resample(self, A[..., 3], out_shape, t, alpha=alpha)
            output = _resample(self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)
            output[..., 3] = output_alpha
        output = self.to_rgba(output, bytes=True, norm=False)
        if A.ndim == 2:
            alpha = self._get_scalar_alpha()
            alpha_channel = output[:, :, 3]
            alpha_channel[:] = np.asarray(np.asarray(alpha_channel, np.float32) * out_alpha * alpha, np.uint8)
    else:
        if self._imcache is None:
            self._imcache = self.to_rgba(A, bytes=True, norm=A.ndim == 2)
        output = self._imcache
        subset = TransformedBbox(clip_bbox, t0.inverted()).frozen()
        output = output[int(max(subset.ymin, 0)):int(min(subset.ymax + 1, output.shape[0])), int(max(subset.xmin, 0)):int(min(subset.xmax + 1, output.shape[1]))]
        t = Affine2D().translate(int(max(subset.xmin, 0)), int(max(subset.ymin, 0))) + t
    return (output, clipped_bbox.x0, clipped_bbox.y0, t)","def normalize_rescale_and_transform_image(self, A, in_bbox, out_bbox, clip_bbox, magnification=1.0, unsampled=False, round_to_pixel_border=True):
    """"""
        Normalize, rescale, and colormap the image *A* from the given *in_bbox*
        (in data space), to the given *out_bbox* (in pixel space) clipped to
        the given *clip_bbox* (also in pixel space), and magnified by the
        *magnification* factor.

        *A* may be a greyscale image (M, N) with a dtype of float32, float64,
        float128, uint16 or uint8, or an (M, N, 4) RGBA image with a dtype of
        float32, float64, float128, or uint8.

        If *unsampled* is True, the image will not be scaled, but an
        appropriate affine transformation will be returned instead.

        If *round_to_pixel_border* is True, the output image size will be
        rounded to the nearest pixel boundary.  This makes the images align
        correctly with the axes.  It should not be used if exact scaling is
        needed, such as for `FigureImage`.

        Returns
        -------
        image : (M, N, 4) uint8 array
            The RGBA image, resampled unless *unsampled* is True.
        x, y : float
            The upper left corner where the image should be drawn, in pixel
            space.
        trans : Affine2D
            The affine transformation from image to pixel space.
        """"""
    if A is None:
        raise RuntimeError('You must first set the image array or the image attribute')
    if A.size == 0:
        raise RuntimeError(""_make_image must get a non-empty image. Your Artist's draw method must filter before this method is called."")
    clipped_bbox = Bbox.intersection(out_bbox, clip_bbox)
    if clipped_bbox is None:
        return (None, 0, 0, None)
    out_width_base = clipped_bbox.width * magnification
    out_height_base = clipped_bbox.height * magnification
    if out_width_base == 0 or out_height_base == 0:
        return (None, 0, 0, None)
    if self.origin == 'upper':
        t0 = Affine2D().translate(0, -A.shape[0]).scale(1, -1)
    else:
        t0 = IdentityTransform()
    t0 += Affine2D().scale(in_bbox.width / A.shape[1], in_bbox.height / A.shape[0]).translate(in_bbox.x0, in_bbox.y0) + self.get_transform()
    t = t0 + Affine2D().translate(-clipped_bbox.x0, -clipped_bbox.y0).scale(magnification)
    if t.is_affine and round_to_pixel_border and (out_width_base % 1.0 != 0.0 or out_height_base % 1.0 != 0.0):
        out_width = math.ceil(out_width_base)
        out_height = math.ceil(out_height_base)
        extra_width = (out_width - out_width_base) / out_width_base
        extra_height = (out_height - out_height_base) / out_height_base
        t += Affine2D().scale(1.0 + extra_width, 1.0 + extra_height)
    else:
        out_width = int(out_width_base)
        out_height = int(out_height_base)
    out_shape = (out_height, out_width)
    if not unsampled:
        if not (A.ndim == 2 or (A.ndim == 3 and A.shape[-1] in (3, 4))):
            raise ValueError(f'Invalid shape {A.shape} for image data')
        if A.ndim == 2:
            inp_dtype = A.dtype
            a_min = A.min()
            a_max = A.max()
            if a_min is np.ma.masked:
                a_min, a_max = (np.int32(0), np.int32(1))
            if inp_dtype.kind == 'f':
                scaled_dtype = np.dtype(np.float64 if A.dtype.itemsize > 4 else np.float32)
                if scaled_dtype.itemsize < A.dtype.itemsize:
                    _api.warn_external(f'Casting input data from {A.dtype} to {scaled_dtype} for imshow')
            else:
                da = a_max.astype(np.float64) - a_min.astype(np.float64)
                scaled_dtype = np.float64 if da > 100000000.0 else np.float32
            A_scaled = np.array(A, dtype=scaled_dtype)
            self.norm.autoscale_None(A)
            dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)
            vmid = np.float64(self.norm.vmin) + dv / 2
            fact = 10000000.0 if scaled_dtype == np.float64 else 10000.0
            newmin = vmid - dv * fact
            if newmin < a_min:
                newmin = None
            else:
                a_min = np.float64(newmin)
            newmax = vmid + dv * fact
            if newmax > a_max:
                newmax = None
            else:
                a_max = np.float64(newmax)
            if newmax is not None or newmin is not None:
                np.clip(A_scaled, newmin, newmax, out=A_scaled)
            offset = 0.1
            frac = 0.8
            vmin, vmax = (self.norm.vmin, self.norm.vmax)
            if vmin is np.ma.masked:
                vmin, vmax = (a_min, a_max)
            vrange = np.array([vmin, vmax], dtype=scaled_dtype)
            A_scaled -= a_min
            vrange -= a_min
            a_min = a_min.astype(scaled_dtype).item()
            a_max = a_max.astype(scaled_dtype).item()
            if a_min != a_max:
                A_scaled /= (a_max - a_min) / frac
                vrange /= (a_max - a_min) / frac
            A_scaled += offset
            vrange += offset
            A_resampled = _resample(self, A_scaled, out_shape, t)
            del A_scaled
            A_resampled -= offset
            vrange -= offset
            if a_min != a_max:
                A_resampled *= (a_max - a_min) / frac
                vrange *= (a_max - a_min) / frac
            A_resampled += a_min
            vrange += a_min
            if isinstance(self.norm, mcolors.NoNorm):
                A_resampled = A_resampled.astype(A.dtype)
            mask = np.where(A.mask, np.float32(np.nan), np.float32(1)) if A.mask.shape == A.shape else np.ones_like(A, np.float32)
            out_alpha = _resample(self, mask, out_shape, t, resample=True)
            del mask
            out_mask = np.isnan(out_alpha)
            out_alpha[out_mask] = 1
            alpha = self.get_alpha()
            if alpha is not None and np.ndim(alpha) > 0:
                out_alpha *= _resample(self, alpha, out_shape, t, resample=True)
            resampled_masked = np.ma.masked_array(A_resampled, out_mask)
            s_vmin, s_vmax = vrange
            if isinstance(self.norm, mcolors.LogNorm):
                if s_vmin < 0:
                    s_vmin = max(s_vmin, np.finfo(scaled_dtype).eps)
            with cbook._setattr_cm(self.norm, vmin=s_vmin, vmax=s_vmax):
                output = self.norm(resampled_masked)
        else:
            if A.shape[2] == 3:
                A = _rgb_to_rgba(A)
            alpha = self._get_scalar_alpha()
            output_alpha = _resample(self, A[..., 3], out_shape, t, alpha=alpha)
            output = _resample(self, _rgb_to_rgba(A[..., :3]), out_shape, t, alpha=alpha)
            output[..., 3] = output_alpha
        output = self.to_rgba(output, bytes=True, norm=False)
        if A.ndim == 2:
            alpha = self._get_scalar_alpha()
            alpha_channel = output[:, :, 3]
            alpha_channel[:] = np.asarray(np.asarray(alpha_channel, np.float32) * out_alpha * alpha, np.uint8)
    else:
        if self._imcache is None:
            self._imcache = self.to_rgba(A, bytes=True, norm=A.ndim == 2)
        output = self._imcache
        subset = TransformedBbox(clip_bbox, t0.inverted()).frozen()
        output = output[int(max(subset.ymin, 0)):int(min(subset.ymax + 1, output.shape[0])), int(max(subset.xmin, 0)):int(min(subset.xmax + 1, output.shape[1]))]
        t = Affine2D().translate(int(max(subset.xmin, 0)), int(max(subset.ymin, 0))) + t
    return (output, clipped_bbox.x0, clipped_bbox.y0, t)","[{""var"": ""da"", ""rename"": ""dynamic_range_adjustment""}, {""var"": ""A_resampled"", ""rename"": ""A_resampled_image""}, {""var"": ""extra_width"", ""rename"": ""additionalWidthRatio""}, {""var"": ""out_shape"", ""rename"": ""output_image_dimensions""}, {""var"": ""vmid"", ""rename"": ""normalized_value_midpoint""}, {""var"": ""t"", ""rename"": ""affine_transform""}, {""var"": ""A_scaled"", ""rename"": ""normalized_rescaled_image""}, {""var"": ""extra_height"", ""rename"": ""height_scaling_adjustment""}, {""var"": ""out_height"", ""rename"": ""output_image_height""}, {""var"": ""frac"", ""rename"": ""normalization_factor""}, {""var"": ""subset"", ""rename"": ""clipped_transformed_bbox""}, {""var"": ""out_mask"", ""rename"": ""is_out_alpha_nan_mask""}, {""var"": ""vmax"", ""rename"": ""value_maximum_threshold""}, {""var"": ""vrange"", ""rename"": ""value_range""}, {""var"": ""dv"", ""rename"": ""value_range_difference""}, {""var"": ""a_max"", ""rename"": ""image_max_value""}, {""var"": ""s_vmax"", ""rename"": ""normalized_value_max""}, {""var"": ""t0"", ""rename"": ""initial_transform""}, {""var"": ""alpha"", ""rename"": ""image_alpha_transparency""}, {""var"": ""out_width_base"", ""rename"": ""magnified_output_width""}, {""var"": ""mask"", ""rename"": ""image_mask""}, {""var"": ""newmin"", ""rename"": ""adjusted_minimum_value""}, {""var"": ""offset"", ""rename"": ""brightness_offset""}, {""var"": ""a_min"", ""rename"": ""min_pixel_value""}, {""var"": ""out_width"", ""rename"": ""output_image_width""}, {""var"": ""fact"", ""rename"": ""scaling_factor_adjustment""}, {""var"": ""inp_dtype"", ""rename"": ""input_image_dtype""}, {""var"": ""out_height_base"", ""rename"": ""magnified_out_height""}, {""var"": ""clipped_bbox"", ""rename"": ""intersection_bbox""}, {""var"": ""vmin"", ""rename"": ""value_minimum""}, {""var"": ""scaled_dtype"", ""rename"": ""normalized_dtype""}, {""var"": ""out_alpha"", ""rename"": ""output_alpha_mask""}, {""var"": ""resampled_masked"", ""rename"": ""masked_resampled_image""}, {""var"": ""output_alpha"", ""rename"": ""resampled_alpha_channel""}, {""var"": ""output"", ""rename"": ""normalized_rescaled_rgba_image""}, {""var"": ""alpha_channel"", ""rename"": ""transparency_layer""}, {""var"": ""newmax"", ""rename"": ""rescaled_maximum_value""}, {""var"": ""s_vmin"", ""rename"": ""scaled_vmin""}]"
