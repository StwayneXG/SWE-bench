file,method_name,new_method_name,start_line,end_line,original_code,code,var
./sklearn/ensemble/_iforest.py,fit,initialize_and_fit_estimator,268,350,"def fit(self, X, y=None, sample_weight=None):
    """"""
        Fit estimator.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency. Sparse matrices are also supported, use sparse
            ``csc_matrix`` for maximum efficiency.

        y : Ignored
            Not used, present for API consistency by convention.

        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights. If None, then samples are equally weighted.

        Returns
        -------
        self : object
            Fitted estimator.
        """"""
    self._validate_params()
    X = self._validate_data(X, accept_sparse=['csc'], dtype=tree_dtype)
    if issparse(X):
        X.sort_indices()
    rnd = check_random_state(self.random_state)
    y = rnd.uniform(size=X.shape[0])
    n_samples = X.shape[0]
    if isinstance(self.max_samples, str) and self.max_samples == 'auto':
        max_samples = min(256, n_samples)
    elif isinstance(self.max_samples, numbers.Integral):
        if self.max_samples > n_samples:
            warn('max_samples (%s) is greater than the total number of samples (%s). max_samples will be set to n_samples for estimation.' % (self.max_samples, n_samples))
            max_samples = n_samples
        else:
            max_samples = self.max_samples
    else:
        max_samples = int(self.max_samples * X.shape[0])
    self.max_samples_ = max_samples
    max_depth = int(np.ceil(np.log2(max(max_samples, 2))))
    super()._fit(X, y, max_samples, max_depth=max_depth, sample_weight=sample_weight, check_input=False)
    self._average_path_length_per_tree, self._decision_path_lengths = zip(*[(_average_path_length(tree.tree_.n_node_samples), tree.tree_.compute_node_depths()) for tree in self.estimators_])
    if self.contamination == 'auto':
        self.offset_ = -0.5
        return self
    self.offset_ = np.percentile(self.score_samples(X), 100.0 * self.contamination)
    return self","def initialize_and_fit_estimator(self, X, y=None, sample_weight=None):
    """"""
        Fit estimator.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples. Use ``dtype=np.float32`` for maximum
            efficiency. Sparse matrices are also supported, use sparse
            ``csc_matrix`` for maximum efficiency.

        y : Ignored
            Not used, present for API consistency by convention.

        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights. If None, then samples are equally weighted.

        Returns
        -------
        self : object
            Fitted estimator.
        """"""
    self._validate_params()
    X = self._validate_data(X, accept_sparse=['csc'], dtype=tree_dtype)
    if issparse(X):
        X.sort_indices()
    rnd = check_random_state(self.random_state)
    y = rnd.uniform(size=X.shape[0])
    n_samples = X.shape[0]
    if isinstance(self.max_samples, str) and self.max_samples == 'auto':
        max_samples = min(256, n_samples)
    elif isinstance(self.max_samples, numbers.Integral):
        if self.max_samples > n_samples:
            warn('max_samples (%s) is greater than the total number of samples (%s). max_samples will be set to n_samples for estimation.' % (self.max_samples, n_samples))
            max_samples = n_samples
        else:
            max_samples = self.max_samples
    else:
        max_samples = int(self.max_samples * X.shape[0])
    self.max_samples_ = max_samples
    max_depth = int(np.ceil(np.log2(max(max_samples, 2))))
    super()._fit(X, y, max_samples, max_depth=max_depth, sample_weight=sample_weight, check_input=False)
    self._average_path_length_per_tree, self._decision_path_lengths = zip(*[(_average_path_length(tree.tree_.n_node_samples), tree.tree_.compute_node_depths()) for tree in self.estimators_])
    if self.contamination == 'auto':
        self.offset_ = -0.5
        return self
    self.offset_ = np.percentile(self.score_samples(X), 100.0 * self.contamination)
    return self","[{""var"": ""tree"", ""rename"": ""estimator_tree""}, {""var"": ""n_samples"", ""rename"": ""num_input_samples""}, {""var"": ""rnd"", ""rename"": ""random_state_generator""}, {""var"": ""max_samples"", ""rename"": ""effective_sample_limit""}, {""var"": ""max_depth"", ""rename"": ""maximum_tree_depth""}]"
./sklearn/ensemble/_iforest.py,score_samples,compute_anomaly_inverse_scores,407,440,"def score_samples(self, X):
    """"""
        Opposite of the anomaly score defined in the original paper.

        The anomaly score of an input sample is computed as
        the mean anomaly score of the trees in the forest.

        The measure of normality of an observation given a tree is the depth
        of the leaf containing this observation, which is equivalent to
        the number of splittings required to isolate this point. In case of
        several observations n_left in the leaf, the average path length of
        a n_left samples isolation tree is added.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        scores : ndarray of shape (n_samples,)
            The anomaly score of the input samples.
            The lower, the more abnormal.
        """"""
    check_is_fitted(self)
    X = self._validate_data(X, accept_sparse='csr', dtype=np.float32, reset=False)
    return -self._compute_chunked_score_samples(X)","def compute_anomaly_inverse_scores(self, X):
    """"""
        Opposite of the anomaly score defined in the original paper.

        The anomaly score of an input sample is computed as
        the mean anomaly score of the trees in the forest.

        The measure of normality of an observation given a tree is the depth
        of the leaf containing this observation, which is equivalent to
        the number of splittings required to isolate this point. In case of
        several observations n_left in the leaf, the average path length of
        a n_left samples isolation tree is added.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            The input samples.

        Returns
        -------
        scores : ndarray of shape (n_samples,)
            The anomaly score of the input samples.
            The lower, the more abnormal.
        """"""
    check_is_fitted(self)
    X = self._validate_data(X, accept_sparse='csr', dtype=np.float32, reset=False)
    return -self._compute_chunked_score_samples(X)",[]
./sklearn/ensemble/_iforest.py,_compute_chunked_score_samples,_compute_chunked_feature_scores,442,472,"def _compute_chunked_score_samples(self, X):
    n_samples = _num_samples(X)
    if self._max_features == X.shape[1]:
        subsample_features = False
    else:
        subsample_features = True
    chunk_n_rows = get_chunk_n_rows(row_bytes=16 * self._max_features, max_n_rows=n_samples)
    slices = gen_batches(n_samples, chunk_n_rows)
    scores = np.zeros(n_samples, order='f')
    for sl in slices:
        scores[sl] = self._compute_score_samples(X[sl], subsample_features)
    return scores","def _compute_chunked_feature_scores(self, X):
    n_samples = _num_samples(X)
    if self._max_features == X.shape[1]:
        subsample_features = False
    else:
        subsample_features = True
    chunk_n_rows = get_chunk_n_rows(row_bytes=16 * self._max_features, max_n_rows=n_samples)
    slices = gen_batches(n_samples, chunk_n_rows)
    scores = np.zeros(n_samples, order='f')
    for sl in slices:
        scores[sl] = self._compute_score_samples(X[sl], subsample_features)
    return scores","[{""var"": ""n_samples"", ""rename"": ""total_data_points""}, {""var"": ""sl"", ""rename"": ""sample_slice""}, {""var"": ""slices"", ""rename"": ""data_chunk_slices""}, {""var"": ""subsample_features"", ""rename"": ""should_subsample_features""}, {""var"": ""scores"", ""rename"": ""chunked_score_samples""}, {""var"": ""chunk_n_rows"", ""rename"": ""chunk_row_limit""}]"
